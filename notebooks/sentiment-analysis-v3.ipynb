{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11075868,"sourceType":"datasetVersion","datasetId":6902828},{"sourceId":11088147,"sourceType":"datasetVersion","datasetId":6911294}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n    set_seed\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport argparse\nfrom tqdm import tqdm\nimport random\n\n\nMODEL_NAME = \"lordtt13/emo-mobilebert\"\n#MODEL_NAME = \"JuliusAlphonso/distilbert-plutchik\"\n#DATASET_PATH = \"/kaggle/input/dataset-5/dataset.csv\"\n#TEXT_COLUMN = \"TESTO\"\n#LABEL_COLUMN = \"EMOZIONI\"\nOUTPUT_DIR = \"/kaggle/working/best_model\"\nBATCH_SIZE = 32\nEPOCHS = 50\nLEARNING_RATE = 2e-5\nMAX_LENGTH = 128\nSEED = 12\n\n# Reproducibility\nset_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Dispositivo in uso: {device}\")\n\n# Custom Dataset\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\ndef load_data(file_path, text_col, label_col):\n    \"\"\"Carica il dataset da un file CSV.\"\"\"\n    df = pd.read_csv(file_path)\n    \n    # Verifica la presenza delle colonne necessarie\n    if text_col not in df.columns or label_col not in df.columns:\n        available_cols = \", \".join(df.columns)\n        raise ValueError(f\"Colonne richieste non trovate. Colonne disponibili: {available_cols}\")\n    \n    # Se le etichette sono testuali, convertiamole in numeriche\n    if not pd.api.types.is_numeric_dtype(df[label_col]):\n        label_map = {label: idx for idx, label in enumerate(df[label_col].unique())}\n        df['label_id'] = df[label_col].map(label_map)\n        print(f\"Mappatura etichette: {label_map}\")\n        return df[text_col].values, df['label_id'].values, label_map\n    \n    return df[text_col].values, df[label_col].values, None\n\ndef train_epoch(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    progress_bar = tqdm(data_loader, desc=\"Training\")\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n            \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().numpy())\n        \n        progress_bar.set_postfix({\"loss\": loss.item()})\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Validation\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef optimize_for_raspberry_pi(model, tokenizer, output_dir):\n    # Convert to quantized model to reduce size and improve inference speed\n    # Use torch.quantization for 8-bit quantization\n    quantized_model = torch.quantization.quantize_dynamic(\n        model, {torch.nn.Linear}, dtype=torch.qint8\n    )\n    \n    # Save the quantized model\n    torch.save(quantized_model.state_dict(), f\"{output_dir}/mobilebert_sentiment_quantized.pt\")\n    \n    # Save the tokenizer\n    tokenizer.save_pretrained(output_dir)\n    \n    # Export to ONNX for better performance (optional)\n    dummy_input = torch.randint(1, 10000, (1, 128)).to('cuda')\n    torch.onnx.export(\n        model, \n        dummy_input, \n        f\"{output_dir}/mobilebert_sentiment.onnx\",\n        export_params=True,\n        opset_version=11,\n        input_names=['input'],\n        output_names=['output'],\n        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n    )\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Carica il dataset\n    print(f\"Caricamento del dataset...\")\n    dataset = load_dataset(\"MelmaGrigia/italian-text-sentiment-analysis\")\n    #texts, labels, label_map = load_data(DATASET_PATH, TEXT_COLUMN, LABEL_COLUMN)\n    \n    # Divisione in training e validation set\n    #train_texts, val_texts, train_labels, val_labels = train_test_split(\n    #    texts, labels, test_size=0.2, random_state=SEED\n    #)\n\n    print(f\"Struttura del dataset: {dataset}\")\n    print(f\"Colonne: {dataset['train'].column_names}\")\n    \n    # Estrai i testi e le etichette\n    train_texts = dataset['train']['text']\n    train_labels = dataset['train']['label']\n\n    val_texts = dataset['test']['text']\n    val_labels = dataset['test']['label']\n\n    num_labels = len(set(train_labels))\n    print(f\"Numero di etichette: {num_labels}\")\n    \n    # Calcola la distribuzione delle classi\n    class_counts = np.bincount(train_labels)\n    print(f\"Distribuzione delle classi: {class_counts}\")\n    \n    print(f\"Testi di training: {len(train_texts)}\")\n    print(f\"Testi di validazione: {len(val_texts)}\")\n\n    # Print class distribution\n    #class_counts = np.bincount(labels)\n    #print(\"Class distribution:\", class_counts)\n    \n    # You might need class weights\n    #class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n    #class_weights = class_weights / class_weights.sum()\n    #class_weights = class_weights.to(device)\n    #print(\"Pesi delle classi:\", class_weights)\n    \n    # Then in your loss calculation:\n    #loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n    \n    # Carica il tokenizer e il modello\n    print(f\"Caricamento del modello {MODEL_NAME}...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=num_labels,\n        ignore_mismatched_sizes=True  \n    )\n\n    # Freeze all parameters except the classifier\n    #for param in model.parameters():\n    #    param.requires_grad = False\n    \n    # Unfreeze only the classifier parameters\n    #for param in model.classifier.parameters():\n    #    param.requires_grad = True\n\n    # Prepara i dataset\n    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n    \n    # Prepara i dataloader\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    \n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n    \n    # Prepara l'ottimizzatore e lo scheduler\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) \n    #optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=1e-4)\n        \n    total_steps = len(train_dataloader) * EPOCHS\n    #warmup_steps = int(0.1 * total_steps)\n    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n    #scheduler = get_linear_schedule_with_warmup(\n    #    optimizer,\n    #    num_warmup_steps=warmup_steps,\n    #    num_training_steps=total_steps\n    #)\n    \n    # Training\n    print(\"Inizio dell'addestramento...\")\n    best_val_f1 = 0.0\n    patience = 5\n    patience_counter = 0\n\n    model.to(device)\n    \n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoca {epoch+1}/{EPOCHS}\")\n        \n        train_loss, train_acc, train_f1 = train_epoch(\n            model, train_dataloader, optimizer, scheduler, device\n        )\n        \n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n        \n        val_loss, val_acc, val_f1 = evaluate(\n            model, val_dataloader, device\n        )\n        \n        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n\n        # Salva il modello se abbiamo ottenuto un miglior F1 score\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            patience_counter = 0\n            \n            # Salva il modello\n            output_path = 'best_model'\n            model.save_pretrained(output_path)\n            tokenizer.save_pretrained(output_path)\n            print(f\"Modello salvato in {output_path}\")\n\n        \n        #else:\n        #    patience_counter += 1\n        #    print(f\"Early stopping patience: {patience_counter}/{patience}\")\n            \n        #    if patience_counter >= patience:\n        #        print(\"Early stopping attivato.\")\n        #        break\n    \n    print(\"\\nAddestramento completato!\")\n    print(f\"Miglior F1 score di validazione: {best_val_f1:.4f}\")\n    optimize_for_raspberry_pi(model, tokenizer, OUTPUT_DIR)\n\n    print(\"\\nOptimization completed!\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load your pre-trained quantized model and tokenizer\nmodel_path = \"/kaggle/working/best_model\"\ntokenizer_path = \"/kaggle/working/best_model\"\n\n# Load the quantized model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Use the tokenizer to create a valid dummy input\nsample_text = \"This is a sample input for the model.\"\ninputs = tokenizer(sample_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n\n# Extract the input_ids tensor\ndummy_input = inputs[\"input_ids\"]\n\n# Export the model to ONNX format\noutput_onnx_path = \"/kaggle/working/best_model/mobilebert_sentiment.onnx\"\ntorch.onnx.export(\n    model,\n    dummy_input,\n    output_onnx_path,\n    export_params=True,  # Store the trained parameter weights inside the model file\n    opset_version=11,    # The ONNX version to export the model to\n    input_names=[\"input_ids\"],  # The model's input names\n    output_names=[\"output\"],    # The model's output names\n    dynamic_axes={\n        \"input_ids\": {0: \"batch_size\"},  # Dynamic axes for input (batch size)\n        \"output\": {0: \"batch_size\"},    # Dynamic axes for output (batch size)\n    },\n    do_constant_folding=True,  # Optimize the model by folding constants\n)\n\nprint(f\"Model has been exported to {output_onnx_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T13:01:41.157715Z","iopub.execute_input":"2025-03-19T13:01:41.158027Z","iopub.status.idle":"2025-03-19T13:01:45.037933Z","shell.execute_reply.started":"2025-03-19T13:01:41.158005Z","shell.execute_reply":"2025-03-19T13:01:45.037188Z"}},"outputs":[{"name":"stdout","text":"Model has been exported to /kaggle/working/best_model/mobilebert_sentiment.onnx\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}