{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11075868,"sourceType":"datasetVersion","datasetId":6902828},{"sourceId":11088147,"sourceType":"datasetVersion","datasetId":6911294}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n    set_seed\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport argparse\nfrom tqdm import tqdm\nimport random\n\n\nMODEL_NAME = \"lordtt13/emo-mobilebert\"\n#MODEL_NAME = \"JuliusAlphonso/distilbert-plutchik\"\nDATASET_PATH = \"/kaggle/input/dataset-5/dataset.csv\"\nTEXT_COLUMN = \"TESTO\"\nLABEL_COLUMN = \"EMOZIONI\"\nOUTPUT_DIR = \"/kaggle/working/best_model\"\nBATCH_SIZE = 32\nEPOCHS = 100\nLEARNING_RATE = 3e-5\nMAX_LENGTH = 128\nSEED = 12\n\n# Reproducibility\nset_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Dispositivo in uso: {device}\")\n\n# Custom Dataset\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            return_token_type_ids=False,\n            return_attention_mask=True,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\ndef load_data(file_path, text_col, label_col):\n    \"\"\"Carica il dataset da un file CSV.\"\"\"\n    df = pd.read_csv(file_path)\n    \n    # Verifica la presenza delle colonne necessarie\n    if text_col not in df.columns or label_col not in df.columns:\n        available_cols = \", \".join(df.columns)\n        raise ValueError(f\"Colonne richieste non trovate. Colonne disponibili: {available_cols}\")\n    \n    # Se le etichette sono testuali, convertiamole in numeriche\n    if not pd.api.types.is_numeric_dtype(df[label_col]):\n        label_map = {label: idx for idx, label in enumerate(df[label_col].unique())}\n        df['label_id'] = df[label_col].map(label_map)\n        print(f\"Mappatura etichette: {label_map}\")\n        return df[text_col].values, df['label_id'].values, label_map\n    \n    return df[text_col].values, df[label_col].values, None\n\ndef train_epoch(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    progress_bar = tqdm(data_loader, desc=\"Training\")\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n            \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().numpy())\n        \n        progress_bar.set_postfix({\"loss\": loss.item()})\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Validation\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef optimize_for_raspberry_pi(model, tokenizer, output_dir):\n    # Convert to quantized model to reduce size and improve inference speed\n    # Use torch.quantization for 8-bit quantization\n    quantized_model = torch.quantization.quantize_dynamic(\n        model, {torch.nn.Linear}, dtype=torch.qint8\n    )\n    \n    # Save the quantized model\n    torch.save(quantized_model.state_dict(), f\"{output_dir}/mobilebert_sentiment_quantized.pt\")\n    \n    # Save the tokenizer\n    tokenizer.save_pretrained(output_dir)\n    \n    # Export to ONNX for better performance (optional)\n    #dummy_input = torch.randint(1, 10000, (1, 128)).to('cuda')\n    #torch.onnx.export(\n    #    model, \n    #    dummy_input, \n    #    f\"{output_dir}/mobilebert_sentiment.onnx\",\n    #    export_params=True,\n    #    opset_version=11,\n    #    input_names=['input'],\n    #    output_names=['output'],\n    #    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n    #)\n\ndef main():\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Carica il dataset\n    print(f\"Caricamento del dataset...\")\n    dataset = load_dataset(\"MelmaGrigia/italian-text-sentiment-analysis\")\n    #texts, labels, label_map = load_data(DATASET_PATH, TEXT_COLUMN, LABEL_COLUMN)\n    \n    # Divisione in training e validation set\n    #train_texts, val_texts, train_labels, val_labels = train_test_split(\n    #    texts, labels, test_size=0.2, random_state=SEED\n    #)\n\n    print(f\"Struttura del dataset: {dataset}\")\n    print(f\"Colonne: {dataset['train'].column_names}\")\n    \n    # Estrai i testi e le etichette\n    train_texts = dataset['train']['text']\n    train_labels = dataset['train']['label']\n\n    val_texts = dataset['test']['text']\n    val_labels = dataset['test']['label']\n\n    num_labels = len(set(train_labels))\n    print(f\"Numero di etichette: {num_labels}\")\n    \n    # Calcola la distribuzione delle classi\n    class_counts = np.bincount(train_labels)\n    print(f\"Distribuzione delle classi: {class_counts}\")\n    \n    print(f\"Testi di training: {len(train_texts)}\")\n    print(f\"Testi di validazione: {len(val_texts)}\")\n\n    # Print class distribution\n    #class_counts = np.bincount(labels)\n    #print(\"Class distribution:\", class_counts)\n    \n    # You might need class weights\n    #class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n    #class_weights = class_weights / class_weights.sum()\n    #class_weights = class_weights.to(device)\n    #print(\"Pesi delle classi:\", class_weights)\n    \n    # Then in your loss calculation:\n    #loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n    \n    # Carica il tokenizer e il modello\n    print(f\"Caricamento del modello {MODEL_NAME}...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=num_labels,\n        ignore_mismatched_sizes=True\n    )\n\n    # Freeze all parameters except the classifier\n    #for param in model.parameters():\n    #    param.requires_grad = False\n    \n    # Unfreeze only the classifier parameters\n    #for param in model.classifier.parameters():\n    #    param.requires_grad = True\n\n    # Prepara i dataset\n    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n    \n    # Prepara i dataloader\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    \n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n    \n    # Prepara l'ottimizzatore e lo scheduler\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) \n    #optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=1e-4)\n        \n    total_steps = len(train_dataloader) * EPOCHS\n    #warmup_steps = int(0.1 * total_steps)\n    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n    #scheduler = get_linear_schedule_with_warmup(\n    #    optimizer,\n    #    num_warmup_steps=warmup_steps,\n    #    num_training_steps=total_steps\n    #)\n    \n    # Training\n    print(\"Inizio dell'addestramento...\")\n    best_val_loss = float('inf')\n    patience = 10\n\n    model.to(device)\n    \n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoca {epoch+1}/{EPOCHS}\")\n        \n        train_loss, train_acc, train_f1 = train_epoch(\n            model, train_dataloader, optimizer, scheduler, device\n        )\n        \n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n        \n        val_loss, val_acc, val_f1 = evaluate(\n            model, val_dataloader, device\n        )\n        \n        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n\n        # Salva il modello se abbiamo ottenuto un miglior F1 score\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience = 10\n            \n            # Salva il modello\n            model.save_pretrained(OUTPUT_DIR)\n            tokenizer.save_pretrained(OUTPUT_DIR)\n            print(f\"Modello salvato in {OUTPUT_DIR}\")\n            \n        else:\n            patience -= 1\n            print(f\"Early stopping patience: {patience}\")\n            \n            if patience == 0:\n                print(\"Early stopping attivato.\")\n                break\n    \n    print(\"\\nAddestramento completato!\")\n    #print(f\"Miglior F1 score di validazione: {best_val_f1:.4f}\")\n    \n    torch.save(model.state_dict(), f\"{OUTPUT_DIR}/mobilebert_sentiment.pt\")\n    optimize_for_raspberry_pi(model, tokenizer, OUTPUT_DIR)\n\n    print(\"\\nOptimization completed!\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T09:21:41.058228Z","iopub.execute_input":"2025-03-24T09:21:41.058583Z","iopub.status.idle":"2025-03-24T09:34:31.647003Z","shell.execute_reply.started":"2025-03-24T09:21:41.058557Z","shell.execute_reply":"2025-03-24T09:34:31.645942Z"}},"outputs":[{"name":"stdout","text":"Dispositivo in uso: cuda\nCaricamento del dataset...\nStruttura del dataset: DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'label_name'],\n        num_rows: 1799\n    })\n    test: Dataset({\n        features: ['text', 'label', 'label_name'],\n        num_rows: 326\n    })\n})\nColonne: ['text', 'label', 'label_name']\nNumero di etichette: 6\nDistribuzione delle classi: [303 302 301 298 298 297]\nTesti di training: 1799\nTesti di validazione: 326\nCaricamento del modello lordtt13/emo-mobilebert...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at lordtt13/emo-mobilebert and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([6]) in the model instantiated\n- classifier.weight: found shape torch.Size([4, 512]) in the checkpoint and torch.Size([6, 512]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Inizio dell'addestramento...\n\nEpoca 1/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.49it/s, loss=1.52]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7470, Accuracy: 0.2279, F1: 0.2083\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.92it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6529, Accuracy: 0.3405, F1: 0.2384\nModello salvato in /kaggle/working/best_model\n\nEpoca 2/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.45it/s, loss=1.35]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5394, Accuracy: 0.3513, F1: 0.3028\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.39it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4379, Accuracy: 0.3957, F1: 0.2788\nModello salvato in /kaggle/working/best_model\n\nEpoca 3/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.40it/s, loss=1.27]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3434, Accuracy: 0.4591, F1: 0.4336\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4528, Accuracy: 0.3497, F1: 0.2459\nEarly stopping patience: 9\n\nEpoca 4/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=1.09] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.1355, Accuracy: 0.5581, F1: 0.5515\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.10it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2646, Accuracy: 0.4540, F1: 0.4044\nModello salvato in /kaggle/working/best_model\n\nEpoca 5/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:15<00:00,  3.57it/s, loss=1.14] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0036, Accuracy: 0.6337, F1: 0.6278\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.11it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.1073, Accuracy: 0.5552, F1: 0.5282\nModello salvato in /kaggle/working/best_model\n\nEpoca 6/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=1.25] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9106, Accuracy: 0.6782, F1: 0.6719\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2935, Accuracy: 0.4816, F1: 0.4313\nEarly stopping patience: 9\n\nEpoca 7/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.42it/s, loss=1.25] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8174, Accuracy: 0.7187, F1: 0.7142\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.79it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.8348, Accuracy: 0.6718, F1: 0.6286\nModello salvato in /kaggle/working/best_model\n\nEpoca 8/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.52it/s, loss=0.403]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6726, Accuracy: 0.7727, F1: 0.7681\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.92it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6539, Accuracy: 0.7423, F1: 0.7007\nModello salvato in /kaggle/working/best_model\n\nEpoca 9/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.55it/s, loss=0.472]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6097, Accuracy: 0.7893, F1: 0.7852\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.01it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6028, Accuracy: 0.7883, F1: 0.7640\nModello salvato in /kaggle/working/best_model\n\nEpoca 10/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.55it/s, loss=0.881]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5533, Accuracy: 0.8049, F1: 0.8027\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.91it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.5083, Accuracy: 0.7883, F1: 0.7753\nModello salvato in /kaggle/working/best_model\n\nEpoca 11/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=1.43] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5135, Accuracy: 0.8277, F1: 0.8260\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.89it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.4375, Accuracy: 0.8712, F1: 0.8677\nModello salvato in /kaggle/working/best_model\n\nEpoca 12/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.51it/s, loss=0.196]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4620, Accuracy: 0.8455, F1: 0.8431\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.96it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.3844, Accuracy: 0.8804, F1: 0.8782\nModello salvato in /kaggle/working/best_model\n\nEpoca 13/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.48it/s, loss=0.594]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4031, Accuracy: 0.8688, F1: 0.8675\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.97it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.3535, Accuracy: 0.8742, F1: 0.8730\nModello salvato in /kaggle/working/best_model\n\nEpoca 14/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.56it/s, loss=0.446]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3435, Accuracy: 0.8949, F1: 0.8940\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.4210, Accuracy: 0.8067, F1: 0.7873\nEarly stopping patience: 9\n\nEpoca 15/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.56it/s, loss=0.244]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3830, Accuracy: 0.8710, F1: 0.8706\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.04it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.3434, Accuracy: 0.8436, F1: 0.8309\nModello salvato in /kaggle/working/best_model\n\nEpoca 16/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.55it/s, loss=0.303]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3403, Accuracy: 0.8866, F1: 0.8862\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.98it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2568, Accuracy: 0.9172, F1: 0.9168\nModello salvato in /kaggle/working/best_model\n\nEpoca 17/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=0.786]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2992, Accuracy: 0.9099, F1: 0.9099\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.95it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2217, Accuracy: 0.9264, F1: 0.9251\nModello salvato in /kaggle/working/best_model\n\nEpoca 18/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=0.0626]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2957, Accuracy: 0.9044, F1: 0.9042\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.3086, Accuracy: 0.8865, F1: 0.8843\nEarly stopping patience: 9\n\nEpoca 19/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.47it/s, loss=0.82]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2577, Accuracy: 0.9300, F1: 0.9299\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.88it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1786, Accuracy: 0.9387, F1: 0.9379\nModello salvato in /kaggle/working/best_model\n\nEpoca 20/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.0445]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2278, Accuracy: 0.9355, F1: 0.9354\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2148, Accuracy: 0.9479, F1: 0.9475\nEarly stopping patience: 9\n\nEpoca 21/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.0487]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2453, Accuracy: 0.9205, F1: 0.9203\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.3614, Accuracy: 0.8681, F1: 0.8664\nEarly stopping patience: 8\n\nEpoca 22/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.0259]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2490, Accuracy: 0.9277, F1: 0.9274\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.93it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1641, Accuracy: 0.9479, F1: 0.9476\nModello salvato in /kaggle/working/best_model\n\nEpoca 23/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.0184]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2125, Accuracy: 0.9339, F1: 0.9337\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.91it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1436, Accuracy: 0.9601, F1: 0.9600\nModello salvato in /kaggle/working/best_model\n\nEpoca 24/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.636] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1835, Accuracy: 0.9489, F1: 0.9488\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2194, Accuracy: 0.9202, F1: 0.9172\nEarly stopping patience: 9\n\nEpoca 25/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.52it/s, loss=0.0247]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1759, Accuracy: 0.9477, F1: 0.9477\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1594, Accuracy: 0.9448, F1: 0.9447\nEarly stopping patience: 8\n\nEpoca 26/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.46it/s, loss=0.0135]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1711, Accuracy: 0.9522, F1: 0.9523\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.82it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1146, Accuracy: 0.9632, F1: 0.9627\nModello salvato in /kaggle/working/best_model\n\nEpoca 27/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=0.0189]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1478, Accuracy: 0.9605, F1: 0.9605\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.87it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1138, Accuracy: 0.9632, F1: 0.9633\nModello salvato in /kaggle/working/best_model\n\nEpoca 28/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=0.0201]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1554, Accuracy: 0.9539, F1: 0.9539\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.96it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1024, Accuracy: 0.9693, F1: 0.9692\nModello salvato in /kaggle/working/best_model\n\nEpoca 29/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.55it/s, loss=0.0242]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1419, Accuracy: 0.9589, F1: 0.9589\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.97it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0730, Accuracy: 0.9785, F1: 0.9785\nModello salvato in /kaggle/working/best_model\n\nEpoca 30/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.56it/s, loss=0.216] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1726, Accuracy: 0.9544, F1: 0.9544\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.96it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0696, Accuracy: 0.9816, F1: 0.9816\nModello salvato in /kaggle/working/best_model\n\nEpoca 31/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.55it/s, loss=0.00891]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1309, Accuracy: 0.9639, F1: 0.9639\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0857, Accuracy: 0.9785, F1: 0.9784\nEarly stopping patience: 9\n\nEpoca 32/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=1.17]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1565, Accuracy: 0.9578, F1: 0.9577\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0932, Accuracy: 0.9755, F1: 0.9752\nEarly stopping patience: 8\n\nEpoca 33/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=0.0137]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1189, Accuracy: 0.9666, F1: 0.9666\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.83it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0510, Accuracy: 0.9816, F1: 0.9816\nModello salvato in /kaggle/working/best_model\n\nEpoca 34/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.48it/s, loss=0.523]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1066, Accuracy: 0.9722, F1: 0.9722\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.89it/s]\n/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:393: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\nNon-default generation parameters: {'max_length': 128}\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0469, Accuracy: 0.9847, F1: 0.9847\nModello salvato in /kaggle/working/best_model\n\nEpoca 35/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.54it/s, loss=0.0106] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1153, Accuracy: 0.9694, F1: 0.9695\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0610, Accuracy: 0.9847, F1: 0.9847\nEarly stopping patience: 9\n\nEpoca 36/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.233]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1093, Accuracy: 0.9739, F1: 0.9739\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0592, Accuracy: 0.9816, F1: 0.9816\nEarly stopping patience: 8\n\nEpoca 37/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.00841]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0888, Accuracy: 0.9750, F1: 0.9750\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0617, Accuracy: 0.9847, F1: 0.9848\nEarly stopping patience: 7\n\nEpoca 38/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.0128] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0743, Accuracy: 0.9805, F1: 0.9805\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0665, Accuracy: 0.9847, F1: 0.9846\nEarly stopping patience: 6\n\nEpoca 39/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.52it/s, loss=1.41]   \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1224, Accuracy: 0.9750, F1: 0.9750\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.2674, Accuracy: 0.9233, F1: 0.9222\nEarly stopping patience: 5\n\nEpoca 40/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.00764]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0789, Accuracy: 0.9767, F1: 0.9766\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0924, Accuracy: 0.9755, F1: 0.9755\nEarly stopping patience: 4\n\nEpoca 41/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.00586]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0934, Accuracy: 0.9750, F1: 0.9749\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1346, Accuracy: 0.9571, F1: 0.9563\nEarly stopping patience: 3\n\nEpoca 42/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.53it/s, loss=0.01]   \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0902, Accuracy: 0.9789, F1: 0.9789\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.1146, Accuracy: 0.9693, F1: 0.9689\nEarly stopping patience: 2\n\nEpoca 43/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.55it/s, loss=0.00599]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0760, Accuracy: 0.9794, F1: 0.9795\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0775, Accuracy: 0.9785, F1: 0.9786\nEarly stopping patience: 1\n\nEpoca 44/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 57/57 [00:16<00:00,  3.55it/s, loss=0.00604]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0568, Accuracy: 0.9850, F1: 0.9850\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00,  9.97it/s]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0672, Accuracy: 0.9785, F1: 0.9786\nEarly stopping patience: 0\nEarly stopping attivato.\n\nAddestramento completato!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-9375b089545b>\u001b[0m in \u001b[0;36m<cell line: 331>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-9375b089545b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAddestramento completato!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Miglior F1 score di validazione: {best_val_f1:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{OUTPUT_DIR}/mobilebert_sentiment.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'best_val_f1' is not defined"],"ename":"NameError","evalue":"name 'best_val_f1' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load your pre-trained quantized model and tokenizer\nmodel_path = \"/kaggle/working/best_model\"\ntokenizer_path = \"/kaggle/working/best_model\"\n\n# Load the quantized model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Use the tokenizer to create a valid dummy input\nsample_text = \"This is a sample input for the model.\"\ninputs = tokenizer(sample_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n\n# Extract the input_ids tensor\ndummy_input = inputs[\"input_ids\"]\n\n# Export the model to ONNX format\noutput_onnx_path = \"/kaggle/working/best_model/mobilebert_sentiment.onnx\"\ntorch.onnx.export(\n    model,\n    dummy_input,\n    output_onnx_path,\n    export_params=True,  # Store the trained parameter weights inside the model file\n    opset_version=11,    # The ONNX version to export the model to\n    input_names=[\"input_ids\"],  # The model's input names\n    output_names=[\"output\"],    # The model's output names\n    dynamic_axes={\n        \"input_ids\": {0: \"batch_size\"},  # Dynamic axes for input (batch size)\n        \"output\": {0: \"batch_size\"},    # Dynamic axes for output (batch size)\n    },\n    do_constant_folding=True,  # Optimize the model by folding constants\n)\n\nprint(f\"Model has been exported to {output_onnx_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Percorso del modello salvato con save_pretrained\nMODEL_DIR = \"/kaggle/working/best_model\"  # OUTPUT_DIR nel tuo codice\nOUTPUT_PATH = \"/kaggle/working/best_model/mobilebert_sentiment.pt\"  # Path per il file .pt\n\n# Carica il modello salvato\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)\n\n# Salva solo lo state_dict in formato .pt\ntorch.save(model.state_dict(), OUTPUT_PATH)\n\nprint(f\"Modello salvato in formato .pt: {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T09:38:48.191995Z","iopub.execute_input":"2025-03-24T09:38:48.192421Z","iopub.status.idle":"2025-03-24T09:38:49.091996Z","shell.execute_reply.started":"2025-03-24T09:38:48.192390Z","shell.execute_reply":"2025-03-24T09:38:49.091170Z"}},"outputs":[{"name":"stdout","text":"Modello salvato in formato .pt: /kaggle/working/best_model/mobilebert_sentiment.pt\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\nquantized_model = torch.quantization.quantize_dynamic(\n        model, {torch.nn.Linear}, dtype=torch.qint8\n    )\n    \n    # Save the quantized model\ntorch.save(quantized_model.state_dict(), f\"{MODEL_DIR}/mobilebert_sentiment_quant.pt\")\n    \n    # Save the tokenizer\n\ntokenizer.save_pretrained(MODEL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T09:47:28.752906Z","iopub.execute_input":"2025-03-24T09:47:28.753261Z","iopub.status.idle":"2025-03-24T09:47:29.471298Z","shell.execute_reply.started":"2025-03-24T09:47:28.753231Z","shell.execute_reply":"2025-03-24T09:47:29.470577Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/best_model/tokenizer_config.json',\n '/kaggle/working/best_model/special_tokens_map.json',\n '/kaggle/working/best_model/vocab.txt',\n '/kaggle/working/best_model/added_tokens.json',\n '/kaggle/working/best_model/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}