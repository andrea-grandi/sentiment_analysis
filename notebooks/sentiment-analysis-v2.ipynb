{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11075868,"sourceType":"datasetVersion","datasetId":6902828}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    AdamW,\n    get_linear_schedule_with_warmup,\n    set_seed\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nimport argparse\nfrom tqdm import tqdm\n\nMODEL_NAME = \"google/mobilebert-uncased\"\nDATASET_PATH = \"/kaggle/input/dataset-sa/MultiEmotions-It.tsv\"\nTEXT_COLUMN = \"comment\"\nLABEL_COLUMN = \"EMOTIONS\"\nOUTPUT_DIR = \"/kaggle/working/\"\nBATCH_SIZE = 32\nEPOCHS = 50\nLEARNING_RATE = 2e-5\nMAX_LENGTH = 128\nSEED = 42\n\n# Impostazioni per la riproducibilità\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Dispositivo in uso: {device}\")\n\n# Definizione della classe per il dataset personalizzato\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\ndef load_data(file_path, text_col, label_col):\n    \"\"\"Carica il dataset da un file TSV.\"\"\"\n    df = pd.read_csv(file_path, sep='\\t')\n    \n    # Verifica la presenza delle colonne necessarie\n    if text_col not in df.columns or label_col not in df.columns:\n        available_cols = \", \".join(df.columns)\n        raise ValueError(f\"Colonne richieste non trovate. Colonne disponibili: {available_cols}\")\n    \n    # Se le etichette sono testuali, convertiamole in numeriche\n    if not pd.api.types.is_numeric_dtype(df[label_col]):\n        label_map = {label: idx for idx, label in enumerate(df[label_col].unique())}\n        df['label_id'] = df[label_col].map(label_map)\n        print(f\"Mappatura etichette: {label_map}\")\n        return df[text_col].values, df['label_id'].values, label_map\n    \n    return df[text_col].values, df[label_col].values, None\n\ndef train_epoch(model, data_loader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    progress_bar = tqdm(data_loader, desc=\"Training\")\n    \n    for batch in progress_bar:\n        optimizer.zero_grad()\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n        loss = outputs.loss\n        total_loss += loss.item()\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().numpy())\n        \n        progress_bar.set_postfix({\"loss\": loss.item()})\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef evaluate(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in tqdm(data_loader, desc=\"Validation\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            loss = outputs.loss\n            total_loss += loss.item()\n            \n            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, accuracy, f1\n\ndef main():\n\n    best_val_f1 = 0.0\n    patience = 5\n    patience_counter = 0\n\n    # Crea la directory di output se non esiste\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    \n    # Carica il dataset\n    print(f\"Caricamento del dataset da {DATASET_PATH}...\")\n    texts, labels, label_map = load_data(DATASET_PATH, TEXT_COLUMN, LABEL_COLUMN)\n    \n    # Divisione in training e validation set\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        texts, labels, test_size=0.1, random_state=SEED\n    )\n    \n    print(f\"Testi di training: {len(train_texts)}\")\n    print(f\"Testi di validazione: {len(val_texts)}\")\n    \n    # Numero di etichette uniche nel dataset\n    num_labels = len(np.unique(labels))\n    print(f\"Numero di etichette: {num_labels}\")\n    \n    # Carica il tokenizer e il modello\n    print(f\"Caricamento del modello {MODEL_NAME}...\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME, num_labels=num_labels, classifier_dropout=0.2 \n    )\n    \n    # Prepara i dataset\n    train_dataset = SentimentDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n    val_dataset = SentimentDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n    \n    # Prepara i dataloader\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True\n    )\n    \n    val_dataloader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=False\n    )\n    \n    # Prepara l'ottimizzatore e lo scheduler\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n    \n    total_steps = len(train_dataloader) * EPOCHS\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n    \n    # Sposta il modello sul dispositivo appropriato\n    model.to(device)\n    \n    # Addestramento\n    print(\"Inizio dell'addestramento...\")\n    best_val_f1 = 0.0\n    \n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoca {epoch+1}/{EPOCHS}\")\n        \n        train_loss, train_acc, train_f1 = train_epoch(\n            model, train_dataloader, optimizer, scheduler, device\n        )\n        \n        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, F1: {train_f1:.4f}\")\n        \n        val_loss, val_acc, val_f1 = evaluate(model, val_dataloader, device)\n        \n        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n\n        # Salva il modello se abbiamo ottenuto un miglior F1 score\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            patient_counter = 0\n            \n            # Salva il modello\n            output_path = os.path.join(OUTPUT_DIR, 'best_model')\n            model.save_pretrained(output_path)\n            tokenizer.save_pretrained(output_path)\n            \n            # Salva la mappatura delle etichette se presente\n            if label_map:\n                label_map_file = os.path.join(output_path, 'label_map.txt')\n                with open(label_map_file, 'w') as f:\n                    for label, idx in label_map.items():\n                        f.write(f\"{label}\\t{idx}\\n\")\n            \n            print(f\"Modello salvato in {output_path}\")\n\n        else:\n            patience_counter += 1 \n            print(f\"Early stopping patience: {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(\"Early stopping attivato: nessun miglioramento per troppe epoche consecutive.\")\n            break\n    \n    print(\"\\nAddestramento completato!\")\n    print(f\"Miglior F1 score di validazione: {best_val_f1:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T15:20:11.495665Z","iopub.execute_input":"2025-03-18T15:20:11.496036Z","iopub.status.idle":"2025-03-18T15:26:42.071879Z","shell.execute_reply.started":"2025-03-18T15:20:11.496006Z","shell.execute_reply":"2025-03-18T15:26:42.070847Z"}},"outputs":[{"name":"stdout","text":"Dispositivo in uso: cuda\nCaricamento del dataset da /kaggle/input/dataset-sa/MultiEmotions-It.tsv...\nMappatura etichette: {'LOVE - DELIGHT': 0, 'TRUST': 1, 'TRUST - SADNESS': 2, 'LOVE': 3, nan: 4, 'OPTIMISM': 5, 'TRUST - DELIGHT': 6, 'DISGUST': 7, 'DISAPPOINTMENT': 8, 'TRUST - OPTIMISM': 9, 'CONTEMPT': 10, 'ANGER': 11, 'LOVE - SENTIMENTALITY': 12, 'JOY': 13, 'DELIGHT': 14, 'LOVE - ANGER': 15, 'TRUST - DISAPPOINTMENT': 16, 'OUTRAGE': 17, 'LOVE - OPTIMISM': 18, 'TRUST - DISGUST': 19, 'TRUST - SURPRISE': 20, 'TRUST - OUTRAGE': 21, 'TRUST - ANTICIPATION': 22, 'LOVE - DISAPPOINTMENT': 23, 'TRUST - ANGER': 24, 'LOVE - ANTICIPATION': 25, 'ANGER - DISAPPOINTMENT': 26, 'SHAME': 27, 'LOVE - SURPRISE': 28, 'TRUST - SENTIMENTALITY': 29, 'SURPRISE': 30, 'SENTIMENTALITY': 31, 'SENTIMENTALITY - ANGER': 32, 'DELIGHT - SADNESS': 33, 'LOVE - SADNESS': 34, 'LOVE - TRUST': 35, 'CURIOSITY': 36, 'LOVE - CURIOSITY': 37, 'SADNESS': 38, 'FEAR': 39, 'ANTICIPATION': 40, 'REMORSE': 41, 'TRUST - JOY': 42, 'PRIDE': 43, 'SENTIMENTALITY - OPTIMISM': 44, 'DISGUST - OPTIMIST': 45, 'PESSIMISM': 46, 'FEAR - PESSIMISM': 47, 'LOVE - SENTIMENTALITY - OPTIMISM': 48, 'SADNESS - DISGUST': 49, 'ANGER - DISGUST': 50, 'SENTIMENTALITY - OUTRAGE': 51, 'ANGER - PESSIMISM': 52, 'JOY - SENTIMENTALITY': 53, 'OPTIMISM - ANGER': 54, 'TRUST - CONTEMPT': 55, 'DESPAIR': 56, 'ANGER - ANTICIPATION': 57, 'SADNESS - ANGER': 58, 'AGGRESSION': 59, 'TRUST - PRIDE': 60, 'JOY - SADNESS': 61, 'FEAR - CURIOSITY': 62, 'ALARM': 63, 'TRUST - FEAR': 64, 'DISGUST - DISAPPOINTMENT': 65, 'TRUST - ANXIETY': 66, 'DISAPPOINTMENT - ANTICIPATION': 67, 'SADNESS - ANTICIPATION': 68, 'TRUST - OPTIMISM - SENTIMENTALITY': 69, 'ANXIETY': 70}\nTesti di training: 2916\nTesti di validazione: 324\nNumero di etichette: 71\nCaricamento del modello google/mobilebert-uncased...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of MobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Inizio dell'addestramento...\n\nEpoca 1/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  3.03it/s, loss=1.72]   \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1845263.1727, Accuracy: 0.0826, F1: 0.0959\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.7892, Accuracy: 0.2438, F1: 0.1286\nModello salvato in /kaggle/working/best_model\n\nEpoca 2/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.99it/s, loss=1.26]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.2038, Accuracy: 0.2130, F1: 0.1572\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.6462, Accuracy: 0.2747, F1: 0.1639\nModello salvato in /kaggle/working/best_model\n\nEpoca 3/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:31<00:00,  2.96it/s, loss=1.59]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.7517, Accuracy: 0.2610, F1: 0.1941\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.4395, Accuracy: 0.3333, F1: 0.2512\nModello salvato in /kaggle/working/best_model\n\nEpoca 4/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:31<00:00,  2.94it/s, loss=3.38]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.6106, Accuracy: 0.3196, F1: 0.2449\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.3534, Accuracy: 0.3704, F1: 0.2997\nModello salvato in /kaggle/working/best_model\n\nEpoca 5/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.98it/s, loss=1.1] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.4062, Accuracy: 0.3642, F1: 0.2938\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.3605, Accuracy: 0.3827, F1: 0.2980\nEarly stopping patience: 1/5\n\nEpoca 6/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.97it/s, loss=1.04]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.7876, Accuracy: 0.3896, F1: 0.3232\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.2480, Accuracy: 0.4105, F1: 0.3415\nModello salvato in /kaggle/working/best_model\n\nEpoca 7/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.98it/s, loss=2.59]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1936, Accuracy: 0.4102, F1: 0.3458\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.2255, Accuracy: 0.4043, F1: 0.3334\nEarly stopping patience: 2/5\n\nEpoca 8/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.97it/s, loss=1.54]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0297, Accuracy: 0.4588, F1: 0.4012\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.2328, Accuracy: 0.4290, F1: 0.3600\nModello salvato in /kaggle/working/best_model\n\nEpoca 9/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:31<00:00,  2.94it/s, loss=3.15]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.4411, Accuracy: 0.4757, F1: 0.4214\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.2039, Accuracy: 0.4537, F1: 0.3961\nModello salvato in /kaggle/working/best_model\n\nEpoca 10/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.98it/s, loss=2.43]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8178, Accuracy: 0.5120, F1: 0.4614\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.2711, Accuracy: 0.4414, F1: 0.3889\nEarly stopping patience: 3/5\n\nEpoca 11/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.97it/s, loss=1.78]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7271, Accuracy: 0.5261, F1: 0.4819\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.2627, Accuracy: 0.4167, F1: 0.3704\nEarly stopping patience: 4/5\n\nEpoca 12/50\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 92/92 [00:30<00:00,  2.98it/s, loss=1.94] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6245, Accuracy: 0.5631, F1: 0.5199\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 11/11 [00:01<00:00, 10.47it/s]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.2933, Accuracy: 0.4352, F1: 0.3891\nEarly stopping patience: 5/5\nEarly stopping attivato: nessun miglioramento per troppe epoche consecutive.\n\nAddestramento completato!\nMiglior F1 score di validazione: 0.3961\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}